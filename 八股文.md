# XGBoost 系列

### [机器学习-XGBoost](https://zhuanlan.zhihu.com/p/105612830)

### [深入理解XGBoost](https://zhuanlan.zhihu.com/p/83901304)

### [XGBoost 20题](https://www.cnblogs.com/cassielcode/p/12469053.html)

### [【白话机器学习】算法理论+实战之Xgboost算法](https://zhuanlan.zhihu.com/p/139635028)

[【务实基础】梯度提升树GBDT](https://zhuanlan.zhihu.com/p/338122487)

​	GBDT 缺点

- 串行方式的模型训练，难并行，造成计算开销。
- 不适合高维稀疏离散特征。这是决策树的痛点，比如动物类别采用one-hot编码后，会产生是否为狗，是否为猫一系列特征，而若这一系列特征中大量样本为狗，其它动物很少，那么树在划分属性时，很容易就划分为“是否为狗”，从而产生过拟合，它不像LR等线性模型f(w,x)的正则化权重是对样本惩罚（可以实现对狗样本给与更大的惩罚项），而树的惩罚项往往是树结构相关的，因此惩罚较小，使得在高维稀疏特征时，GBDT表现不好。

### [【务实基础】XGBoost](https://zhuanlan.zhihu.com/p/340223260)

## BatchNormalization

# Learning to Rank

### [Learning to Rank简介](https://www.cnblogs.com/bentuwuying/p/6681943.html)

### [Learning to Rank算法介绍：GBRank](https://www.cnblogs.com/bentuwuying/p/6684585.html)

### [Learning to Rank算法介绍：RankNet，LambdaRank，LambdaMart ](https://www.cnblogs.com/bentuwuying/p/6690836.html)

# FM系列

# 深度排序

# 粗排

### 框架

#### [阿里粗排技术体系与最新进展分享](https://zhuanlan.zhihu.com/p/355828527)

#### [美团搜索粗排优化的探索与实践](https://zhuanlan.zhihu.com/p/553953132)

#### [升级换代！Facebook全新电商搜索系统Que2Search](https://zhuanlan.zhihu.com/p/428495241)

#### [KDD'21 | 揭秘Facebook升级版语义搜索技术](https://zhuanlan.zhihu.com/p/415516966)

#### [推荐系统粗排之柔：双塔 to NN，Learning to Rank](https://zhuanlan.zhihu.com/p/426679177)

### 样本

#### [召回和粗排负样本构造问题](https://zhuanlan.zhihu.com/p/352961688)

#### [负样本为王：评Facebook的向量化召回算法](https://zhuanlan.zhihu.com/p/165064102)

#### [负样本的艺术，再读Facebook双塔向量召回算法](https://zhuanlan.zhihu.com/p/386913612)

#### [双塔模型中的负采样](https://zhuanlan.zhihu.com/p/406783325)

### 损失函数

#### [召回粗排的损失函数loss function](https://zhuanlan.zhihu.com/p/557416100)

### 评价指标

#### [粗排评价指标(1)](https://zhuanlan.zhihu.com/p/340250384)

#### [看完这篇AUC文章，搞定任何有关AUC的面试不成问题~](https://zhuanlan.zhihu.com/p/360765777)

#### [机器学习的评价指标（一）：Accuracy、Precision、Recall、F1 Score](https://zhuanlan.zhihu.com/p/364253497)

#### [机器学习的评价指标（二）：ROC-AUC和PR-AUC](https://zhuanlan.zhihu.com/p/364299008)

### 其他

[双塔模型存在的问题以及解决方案【综述】](https://zhuanlan.zhihu.com/p/567981161)



# 改写

# 对比学习

# BERT相关