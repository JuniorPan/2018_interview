# XGBoost 系列

#### [机器学习-XGBoost](https://zhuanlan.zhihu.com/p/105612830) [看这个]

#### [深入理解XGBoost](https://zhuanlan.zhihu.com/p/83901304)

#### [XGBoost 20题](https://www.cnblogs.com/cassielcode/p/12469053.html)

#### [【白话机器学习】算法理论+实战之Xgboost算法](https://zhuanlan.zhihu.com/p/139635028) [看这个]

#### [【务实基础】梯度提升树GBDT](https://zhuanlan.zhihu.com/p/338122487)

#### 	GBDT 缺点

- 串行方式的模型训练，难并行，造成计算开销。
- 不适合高维稀疏离散特征。这是决策树的痛点，比如动物类别采用one-hot编码后，会产生是否为狗，是否为猫一系列特征，而若这一系列特征中大量样本为狗，其它动物很少，那么树在划分属性时，很容易就划分为“是否为狗”，从而产生过拟合，它不像LR等线性模型f(w,x)的正则化权重是对样本惩罚（可以实现对狗样本给与更大的惩罚项），而树的惩罚项往往是树结构相关的，因此惩罚较小，使得在高维稀疏特征时，GBDT表现不好。

#### [【务实基础】XGBoost](https://zhuanlan.zhihu.com/p/340223260)

# BatchNormalization

#### [深度学习中的Normalization模型](https://zhuanlan.zhihu.com/p/43200897)

#### [Batch Normalization导读](https://zhuanlan.zhihu.com/p/38176412)

#### [模型优化之Layer Normalization](https://zhuanlan.zhihu.com/p/54530247)

#### [NLP中 batch normalization与 layer normalization](https://zhuanlan.zhihu.com/p/74516930)

#### [超细节的 BatchNorm/BN/LayerNorm/LN 知识点](https://zhuanlan.zhihu.com/p/521535855)( 重点看)

# Learning to Rank

#### [Learning to Rank简介](https://www.cnblogs.com/bentuwuying/p/6681943.html)

#### [Learning to Rank算法介绍：GBRank](https://www.cnblogs.com/bentuwuying/p/6684585.html)

#### [Learning to Rank算法介绍：RankNet，LambdaRank，LambdaMart ](https://www.cnblogs.com/bentuwuying/p/6690836.html)

#### [Learning To Rank 之 RankNet](https://blog.tsingjyujing.com/ml/recsys/ranknet)

# FM系列

#### [FM算法解析](https://zhuanlan.zhihu.com/p/37963267)

**目的：**

旨在解决稀疏数据下的特征组合问题。

**优势：**

- 高度稀疏数据场景；

- 具有线性的计算复杂度。

**特征组合**

多项式模型是包含特征组合的最直观的模型。在多项式模型中，特征$x_i$ 和$x_j$ 的组合采用 $x_ix_j$ 表示，即$x_i$ 和$x_j$ 都非零时，组合特征 $x_ix_j$ 才有意义。从对比的角度，本文只讨论二阶多项式模型。模型的表达式如下
$$
y(X)=\omega_{0}+\sum_{i=1}^{n} \omega_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \omega_{i j} x_{i} x_{j}
$$
FM的模型方程为:
$$
\hat{y}(X):=\omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n<v_i,v_j>x_ix_j

\\ <v_i,v_j>:=\sum_{f=1}^k v_{i,f}\cdot v_{j,f}
$$
公式(2)推导如下
$$
ab+ac+bc=\frac{1}{2}\left[(a+b+c)^2-(a^2+b^2+c^2)\right]\quad
\\ \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}<v_i,v_j>x_ix_j
\\ 	=\dfrac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}<v_i,v_j>x_ix_j-\dfrac{1}{2}\sum_{i=1}^{n}<v_i,v_i>x_ix_i\quad
\\       =\dfrac{1}{2}\left(\sum_{i=1}^n\sum_{j=1}^n\sum_{f=1}^k v_{i,f}v_{j,f}x_ix_j-\sum_{i=1}^n\sum_{f=1}^k v_{i,f}v_{i,f}x_ix_i\right)
\\ =\dfrac{1}{2}\sum_{j=1}^k\left[\left(\sum_{i=1}^n v_{i,f}x_i\right)\cdot\left(\sum_{j=1}^n v_{i,f}x_j\right)-\sum_{i=1}^n v_{i,f}^2x_i^2\right]\quad
\\  =\dfrac{1}{2}\sum_{f=1}^k\left[\left(\sum_{i=1}^n v_{i,f}x_i\right)^2-\sum_{i=1}^n v_{i,f}^2x_i^2\right]\quad
$$
解释：
$v_{i,f}$ 是一个具体的值；
第1个等号：对称矩阵 $W$ 对角线上半部分；
第2个等号：把向量内积 $v_i,v_j$展开成累加和的形式；
第3个等号：提出公共部分；
第4个等号： $i$ 和 $j$ 相当于是一样的，表示成平方过程。



#### [推荐系统中CTR排序模型汇总(LR,FM,FFM,MLR,DCN,deepFM,PNN,AFM等)](https://zhuanlan.zhihu.com/p/268294797)

#### [Deepfm原理和源码 一篇就好](https://zhuanlan.zhihu.com/p/154591869)

#### [DeepFM技术细节](https://zhuanlan.zhihu.com/p/148836639) [重点看这个]

#### [推荐系统召回四模型之：全能的FM模型](https://zhuanlan.zhihu.com/p/58160982)

# 深度排序

#### [连续特征的离散化：在什么情况下将连续的特征离散化之后可以获得更好的效果？](https://www.zhihu.com/question/31989952)

- 离散特征的增加和减少都很容易，易于模型的快速迭代；
- 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
- 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
- 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
- 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
- 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
- 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

**李沐曾经说过**：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

# 召回&粗排

### 框架

#### [阿里粗排技术体系与最新进展分享](https://zhuanlan.zhihu.com/p/355828527)
##### 粗排两大技术路线 
 ###### 集合选择技术
    集合选择技术是以集合为建模目标，选出满足后续链路需求的集合的方法，该技术路线在召回非常典型，这其实也非常符合粗排的定位。该方法优点是算力消耗一般比较少，缺点是依赖于对后链路的学习，可控性较弱。
  ###### 精准值预估技术
    精准值预估技术直接对最终系统目标进行精确值预估，其实也就是pointwise的方式。

**top-k recall**，用于评估粗排和精排的对齐程度。其中top k候选集合和top m候选集合均为粗排的输入打分集合。top k集合是粗排选出的，而 top m集合是精排选出的
$$
\text { recall }=\frac{\mid\{\text { top } k \text { ad candidates }\} \cap \text { top } m \text { ad candidates }\} \mid}{\mid\{\text { top } m \text { ad candidates }\}\} \mid} .
$$


#### [美团搜索粗排优化的探索与实践](https://zhuanlan.zhihu.com/p/553953132)

##### 挑战点

- **样本选择偏差：**级联排序系统下，粗排离最后的结果展示环节较远，导致粗排模型离线训练样本空间与待预测的样本空间存在较大的差异，存在严重的样本选择偏差。
- **粗排精排联动：** 粗排处于召回和精排之间，粗排需要更多获取和利用后续链路的信息来提升效果。
- **性能约束：** 线上粗排预测的候选集远远高于精排模型，然而实际整个搜索系统对性能有严格的要求，导致粗排需要重点关注预测性能。

##### **精排结果列表蒸馏**

```
粗排作为精排的前置模块，它的目标是初步筛选出质量比较好的候选集合进入精排，从训练样本选取来看，除了常规的用户发生行为（点击、下单、支付）的 item 作为正样本，曝光未发生行为的 item 作为负样本外，还可以引入一些通过精排模型排序结果构造的正负样本，这样既能一定程度缓解粗排模型的样本选择偏置，也能将精排的排序能力迁移到粗排
```

- **策略1**：在用户反馈的正负样本基础上，随机选取少量精排排序靠后的未曝光样本作为粗排负样本的补充，如图 3 所示。该项改动离线 Recall@150（指标解释参看附录）+5PP，线上 CTR +0.1%。
- **策略2**：直接在精排排序后的集合里面进行随机采样得到训练样本，精排排序的位置作为 label 构造 pair 对进行训练，如下图 4 所示。离线效果相比策略1 Recall@150 +2PP，线上 CTR +0.06%。
- **策略3**：基于策略2的样本集选取，采用对精排排序位置进行分档构造 label ，然后根据分档 label 构造 pair 对进行训练。离线效果相比策略2 Recall@150 +3PP，线上 CTR +0.1%。

#####  **精排预测分数蒸馏**

- 粗排模型输出的分数与精排模型输出的分数分布尽量对齐

#### [大规模搜索+预训练，百度是如何落地的？](https://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&mid=2247504279&idx=1&sn=8d02ecbde4076932d4d5260379d32780&chksm=97ad3745a0dabe532b697b4407c4f947ce94a5a6a6cae0df534fd290a97fd2bdd2467bd81e86&scene=21#wechat_redirect)

#### [Facebook的搜索是怎么做的？](https://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&mid=2247488092&idx=1&sn=4ebb56be908d229ac5ac36b601991fa3&chksm=97aef48ea0d97d98726fbfb7c575057246be1b994eb501c416228e62d40d63669d0c81afbe9a&scene=21#wechat_redirect)

#### [升级换代！Facebook全新电商搜索系统Que2Search](https://zhuanlan.zhihu.com/p/428495241)

##### 针对『Facebook maketplace』这一特定场景的向量化检索系统，仍然存在以下几点挑战：

- **商品描述存在噪声**：由于商品的属性描述（譬如标题、类别等）是由卖家上传的，会存在较多的拼写错误、属性丢失等；
- **国际化支持**：Facebook Marketplace场景是多国家多语言的，需要模型具备跨语言检索能力；
- **多模态处理**：需要在模型中同时考虑多种模态信息，图片、文本等；
- **严格的延迟性限制**：众所周知线上搜索系统对延迟要求极高，会极大地影响用户体验。尤其是当使用Transformer-based模型时，延迟是一个巨大挑战；

#### [KDD'21 | 揭秘Facebook升级版语义搜索技术](https://zhuanlan.zhihu.com/p/415516966)

#### [推荐系统粗排之柔：双塔 to NN，Learning to Rank](https://zhuanlan.zhihu.com/p/426679177)

#### [Google Play双塔召回算法](https://zhuanlan.zhihu.com/p/533449018)

#### [双塔召回模型的前世今生（上篇）](https://zhuanlan.zhihu.com/p/430503952)

#### [双塔召回模型的前世今生（下篇）](https://zhuanlan.zhihu.com/p/441597009)

#### [久别重逢话双塔](https://zhuanlan.zhihu.com/p/428396126)

**如何采样以减少“样本选择偏差”、如何保证上下游目标一致性、如何在双塔中实现多任务间的信息转移**.

**“双塔召回”与“双塔粗排”所需要的负样本，截然不同**

双塔最大的缺点就在于，**user&item两侧信息交叉得太晚，等到最终能够通过dot或cosine交叉的时候，user & item embedding已经是高度浓缩的了，一些细粒度的信息已经在塔中被损耗掉，永远失去了与对侧信息交叉的机会**。所以，双塔改建最重要的一条主线就是：**如何保留更多的信息在tower的final embedding中，从而有机会和对侧塔得到的embedding交叉？**

### 样本
#### 目标: 拟合精排模型。即粗排尽可能圈出精排模型能排到前面的item。
    对于粗排模型打分比较靠前的样本（比如top5），精排模型却把这部分样本打分很低(比如top250～300)，那么，粗排打分很靠前的这几条样本是不会被曝光的，因此当我们复用精排模型的样本来训练粗排模型时，粗排模型是看不到这几条bad case的样本的，新粗排模型上线后，这几条样本很可能还被粗排模型排到靠前的位置。如果我们把这几条样本作为负样本来训练粗排模型，那么新的模型大概率就可以把这几条样本排到靠后的位置了。
####  离线训练数据的分布，应该与线上实际应用的数据，保持一致 。

排序其目标是“从用户可能喜欢的当中挑选出用户最喜欢的”，是为了优中选优。与召回相比，排序面对的数据环境，简直就是 温室里的花朵 。
召回是“是将用户可能喜欢的，和海量对用户根本不靠谱的，分隔开”，所以召回在线上所面对的数据环境，就是 鱼龙混杂、良莠不齐 。

#### [召回和粗排负样本构造问题](https://zhuanlan.zhihu.com/p/352961688)

#### [负样本为王：评Facebook的向量化召回算法](https://zhuanlan.zhihu.com/p/165064102)

**Pairwise Hinge Loss**: $\operatorname{loss}=\max \left(0, \operatorname{margin}-\operatorname{dot}\left(u, d_{+}\right)+\operatorname{dot}\left(u, d_{-}\right)\right)$

**BPR loss**:  $loss = \log(1+exp(det(u,d_-)-dot(u,d_+)))$

**“双塔召回”与“双塔粗排”所需要的负样本，截然不同**

#### [负样本的艺术，再读Facebook双塔向量召回算法](https://zhuanlan.zhihu.com/p/386913612)

#### [双塔模型中的负采样](https://zhuanlan.zhihu.com/p/406783325)

#### [推荐系统（四）—— 负采样](https://zhuanlan.zhihu.com/p/456088223)

#### [Efficient and Effective: 百篇论文概览负采样方法的前世今生](https://blog.csdn.net/abcdefg90876/article/details/122248083)

### 损失函数

Triplet Loss

#### [召回粗排的损失函数loss function](https://zhuanlan.zhihu.com/p/557416100)

 为什么粗排不用精排一样的pointwise损失函数，看过网上的解释，大致概括为1、粗排不需要像精排一样预估一个有具体含义的概率值，如ctr，只需要把正负例拉开即可。那么pointwise loss也能拉开啊，还真不是：2、pointwise loss有个不足，如第500名和1000名在pointwise 设定里都是负例，label都是0，模型无法区分谁更优秀，但是pairwise的设定可以，500>1000可以作为一对有信息量的pair进行训练。有些人要说，一个有一定正例且训练充分的pointwise模型总归知道500名比1000名强吧，而且pointwise的可能训练起来更稳定好收敛易上手，不一定比pairwise出来的模型差。想知道挖掘机哪家强，还是先好好学开挖掘机

#### [随想： BPR Loss 与 Hinger Loss](https://blog.tsingjyujing.com/ml/recsys/bpr_and_hinger)

### 评价指标

#### [粗排评价指标(1)](https://zhuanlan.zhihu.com/p/340250384)

#### [看完这篇AUC文章，搞定任何有关AUC的面试不成问题~](https://zhuanlan.zhihu.com/p/360765777) [重点看]

#### [机器学习的评价指标（一）：Accuracy、Precision、Recall、F1 Score](https://zhuanlan.zhihu.com/p/364253497)

#### [机器学习的评价指标（二）：ROC-AUC和PR-AUC](https://zhuanlan.zhihu.com/p/364299008)
#### [图解AUC和GAUC](https://zhuanlan.zhihu.com/p/84350940) 【auc 计算】

#### [NDCG排序评估指标](https://zhuanlan.zhihu.com/p/448686098)

### 其他

#### [双塔模型存在的问题以及解决方案【综述】](https://zhuanlan.zhihu.com/p/567981161)

#### [https://www.zhihu.com/question/505942505/answer/2274732251](https://www.zhihu.com/question/505942505/answer/2274732251)

# 多任务学习

#### [代码精读之MMoE](https://zhuanlan.zhihu.com/p/551786848)

#### [mmoe家族的一些summary（待续）](https://zhuanlan.zhihu.com/p/538209695)

#### [推荐系统中的多目标学习](https://zhuanlan.zhihu.com/p/183760759)

# query改写

#### [BART原理简介与代码实战](https://zhuanlan.zhihu.com/p/121788986)

# 对比学习

#### [Contrastive Representation Learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/)

#### [对比学习（Contrastive Learning）综述](https://zhuanlan.zhihu.com/p/346686467)

#### [Contrastive Learning NLP Papers](https://zhuanlan.zhihu.com/p/363900943)

#### [对比学习火了NLP](https://mp.weixin.qq.com/s/EOlXjdd1gCruiF-B1JoIgg)

#### [对比学习（Contrastive Learning）:研究进展精要](https://zhuanlan.zhihu.com/p/367290573)

如何构造相似实例，以及不相似实例，如何构造能够遵循上述指导原则的表示学习模型结构，以及如何防止模型坍塌(Model Collapse)，这几个点是其中的关键

#### [利用Contrastive Learning对抗数据噪声：对比学习在微博场景的实践](https://zhuanlan.zhihu.com/p/370782081)

```
通过LAMB优化器，并结合Gradient Checkpointing优化技巧，放大Batch Size到1664
```

#### [对比学习视角:重新审视推荐系统的召回粗排模型](https://zhuanlan.zhihu.com/p/424198603) [重点看]

```
对比学习它最大的技术源泉来自于度量学习（Metric Learning）,你要看的话,会发现它的运作流程,基本就是度量学习的流程。什么意思呢?度量学习的优化目标就是说：比如我有正例和负例，它要将实体映射到一个空间里面去。它的目标是让正例在空间中近一些，负例在空间中远一些，这是度量学习的一个主体思想
```



    从梯度角度 分析了温度超参数的作用
     
    对比学习它最大的技术源泉来自于度量学习（Metric Learning）,你要看的话,会发现它的运作流程,基本就是度量学习的流程。什么意思呢?度量学习的优化目标就是说：比如我有正例和负例，它要将实体映射到一个空间里面去。它的目标是让正例在空间中近一些，负例在空间中远一些，这是度量学习的一个主体思想
    
    对比学习和度量学习的最大区别在于是否自监督构造训练数据: 具体的区别就是对比学习的正负例是根据一些规则自动构造的，而不是通过人工标注的数据（也就是有监督的方式）


​    

通过分子分母就很容易看出来它的优化目标。**分子部分强调正例，希望它的距离越近越好，分母部分强调负例，希望和负例距离越远越好**


$$
L_{q}=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\left.\sum_{i=0}^{k} \exp \left(q \cdot k_{i} / \tau\right)\right)}
$$



```
什么是模型坍塌？就是说不论你输入的是什么图片，经过映射函数之后，在投影空间里面，所有图像的编码都会坍塌到同一个点。坍塌到同一个点又是什么含义呢？就是说不论我的输入是什么，最终经过函数映射，被映射成同一个embedding，所有图像对应的Embedding都是一样的，这意味着你的映射函数没有编码任何有用的信息，这是一个很致命的问题。所以说怎么避免模型坍塌，是图像领域里面做对比学习非常关键的一个问题。这里讲的是什么是不好的学习系统：容易发生模型坍塌的模型是一个不好的对比学习系统。
```

```
好的对比学习系统应该满足什么条件呢？（可以参考上图所示论文）它应兼顾两个要素： Alignment和Uniformity。

Alignment代表我们希望对比学习把相似的正例在投影空间里面有相近的编码，一般我们做一个embedding映射系统，都是希望达成此目标.
就是这个uniformity。Uniformity代表什么含义呢? Uniformity直观上来说就是：当所有实例映射到投影空间之后，我们希望它在投影空间内的分布是尽可能均匀的。这里有个点不好理解：为什么我们希望分布是均匀的呢？其实，追求分布均匀性不是Uniformity的目的，而是手段。追求分布的Uniformity实际想达成的是什么目标呢？它实际想达成的是:我们希望实例映射到投影空间后，在对应的Embedding包含的信息里，可以更多保留自己个性化的与众不同的信息。

如果你把cosine公式写出来仔细看下的话，会发现cosine可以理解为对user embedding和item embedding各自先做了一个L2 Norm,然后两者再做内积。

于是，问题来了：我们是不是应该对user embedding和item embedding做Norm?大家可能平常不太注意这个点。经验结论是这样的：应该做。意思就是要么你用cosine给它自动做Norm，要么用内积的话，原则上要在前一步做个L2 Norm，然后再去求内积，这两种做法基本等价。
```



```
我个人的理解是这样的：不论做搜索、广告、推荐，用户行为数据的稀疏性是非常严重的，我认为这个问题也是目前制约系统优化很严重的问题。就是说，用户行为数据分布是极度不均匀的，它是个典型的长尾分布，就是真正被用户点过的行为数据item,很多都分布在极少数或者极少比例的item里面，大多数都是长尾的，没有用户数据，或者很少用户数据。这是个很严重的问题。

它可以解决数据长尾分布的问题。对于长尾侧的数据,用现有的有监督方法，你会发现不论是对应的item也好，user也好，还是id特征也好，它打出的embedding不可靠，因为它的频次太低，很难通过很多用户行为数据推导出靠谱的embedding。这时候，对比学习就可以发挥它的作用了，这是推荐领域为什么要引入对比学习的初衷。
```



#### [Noise Contrastive Estimation 前世今生——从 NCE 到 InfoNCE](https://zhuanlan.zhihu.com/p/334772391)

#### [一文掌握《对比学习（Contrastive Learning）》要旨，详述MoCo和SimCLR算法](https://zhuanlan.zhihu.com/p/385160814)

#### [Moco 文章阅读笔记](https://zhuanlan.zhihu.com/p/93996599)

#### [Sentence-BERT: 如何通过对比学习得到更好的句子向量表示](https://zhuanlan.zhihu.com/p/541575049)

#### [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://zhuanlan.zhihu.com/p/368353121)

#### [文本表达：SimCSE、ESimCSE对比与实践](https://zhuanlan.zhihu.com/p/469944640)

#### [【SimCSE】没有标注数据也能训练文本匹配模型（附源码）](https://zhuanlan.zhihu.com/p/599230890)

#### [超细节的对比学习和SimCSE知识点](https://zhuanlan.zhihu.com/p/378340148)

#### [10.文本相似度/语义相似度/文本匹配之对比学习和SimCSE以及InfoNCE loss](https://zhuanlan.zhihu.com/p/569764236)
#### [CVPR2021自监督学习论文: 理解对比损失的性质以及温度系数的作用](https://zhuanlan.zhihu.com/p/357071960)  [重点看]

$$
\mathcal{L}\left(x_{i}\right)=-\log \left[\frac{\exp \left(s_{i, i} / \tau\right)}{\sum_{k \neq i} \exp \left(s_{i, k} / \tau\right)+\exp \left(s_{i, i} / \tau\right)}\right]
$$

$$
\mathcal{L}_{simple}(x_i) =-s_{i,i}+\lambda\sum_{i\neq j}s_{i,j}
$$

$$
对正样本的梯度： \frac{\partial\mathcal{L}(x_i)}{\partial s_{i,i}}=-\frac{1}{\tau}\sum_{k\neq i}P_{i,k}
$$

$$
对负样本的梯度： \frac{\partial\mathcal{L}(x_i)}{\partial s_{i,i}}=-\frac{1}{\tau}\sum_{k\neq i}P_{i,k}
$$

$$
其中 P_{i,j}=\frac{\exp(s_{i,j}/\tau)}{\sum_{k\neq i}\exp(s_{i,k}/\tau)+\exp(s_{i,i}/\tau)}
$$



# BERT相关

#### [This post is all you need（上卷）——层层剥开Transformer](https://zhuanlan.zhihu.com/p/420820453)

#### [BERT中，multi-head 768\*64\*12与直接使用768*768矩阵统一计算，有什么区别？](https://www.zhihu.com/question/446385446/answer/1982483918)

```
区别在于模型容量增加，带来模型表现力的提升。
这里有一个容易引起误解的地方：“多头”不是“加头”，它实际上是“分头”
```

#### [Mask矩阵在深度学习中有哪些应用场景？](https://www.zhihu.com/question/305508138/answer/1215025036)

#### [为什么Transformer要用LayerNorm？](https://www.zhihu.com/question/487766088/answer/2129476576)

#### [transformer问题整理（参考知乎大佬内容）](https://zhuanlan.zhihu.com/p/266695736)

#### [面试准备 transformer及各种周边（待续）](https://zhuanlan.zhihu.com/p/396499248) [重点看]

#### [常用预训练语言模型（PTMs）总结](https://zhuanlan.zhihu.com/p/406512290)[重点看]

#### [tokenizers小结](https://zhuanlan.zhihu.com/p/360290118) [重点看]

#### [史上最细节的自然语言处理NLP/Transformer/BERT/Attention面试问题与答案](https://zhuanlan.zhihu.com/p/348373259)

#### [史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！](https://zhuanlan.zhihu.com/p/148656446)

#### [超细节的BERT/Transformer知识点](https://zhuanlan.zhihu.com/p/132554155)

#### [Transformer总结(2022版)](https://zhuanlan.zhihu.com/p/489187551)

#### [BERT相关面试题（不定期更新）](https://zhuanlan.zhihu.com/p/151412524)

#### [在 BERT 已经成为 NLP 基础知识的当下，你会在面试中问关于 BERT 的哪些问题？](https://www.zhihu.com/question/424003949/answer/2626211073)

#### [原生Bert的训练和使用总结](https://zhuanlan.zhihu.com/p/163239652)

#### [Colbert论文解析](https://zhuanlan.zhihu.com/p/376475610)

#### [BERT生成式之UNILM解读](https://zhuanlan.zhihu.com/p/68327602)

#### [BERT模型蒸馏完全指南（原理/技巧/代码）](https://zhuanlan.zhihu.com/p/273378905)

#### [如何看待瘦身成功版BERT——ALBERT？](https://www.zhihu.com/question/347898375/answer/863537122)

#### [Transformer中使用的position embedding为什么是加法而不是concat](https://blog.csdn.net/weixin_38224810/article/details/124790477)

# NLP

#### [深入理解NLP Subword算法：BPE、WordPiece、ULM](https://zhuanlan.zhihu.com/p/86965595)

#### [NLP三大Subword模型详解：BPE、WordPiece、ULM](https://zhuanlan.zhihu.com/p/198964217)

#### [理解NLP最重要的编码方式 — Byte Pair Encoding (BPE)，这一篇就够了](https://zhuanlan.zhihu.com/p/424631681)

#### [NLP领域中的token和tokenization到底指的是什么？](https://www.zhihu.com/question/64984731/answer/3010175256)

# 机器学习

#### [关于损失函数和优化算法，看这一篇就够了](https://zhuanlan.zhihu.com/p/149715152)

#### [PyTorch的十八个损失函数](https://zhuanlan.zhihu.com/p/61379965)

#### [How to Train Really Large Models on Many GPUs?](https://lilianweng.github.io/posts/2021-09-25-train-large/) 非常重要



