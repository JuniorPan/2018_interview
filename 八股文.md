# XGBoost 系列

#### [机器学习-XGBoost](https://zhuanlan.zhihu.com/p/105612830)

#### [深入理解XGBoost](https://zhuanlan.zhihu.com/p/83901304)

#### [XGBoost 20题](https://www.cnblogs.com/cassielcode/p/12469053.html)

#### [【白话机器学习】算法理论+实战之Xgboost算法](https://zhuanlan.zhihu.com/p/139635028)

#### [【务实基础】梯度提升树GBDT](https://zhuanlan.zhihu.com/p/338122487)

#### 	GBDT 缺点

- 串行方式的模型训练，难并行，造成计算开销。
- 不适合高维稀疏离散特征。这是决策树的痛点，比如动物类别采用one-hot编码后，会产生是否为狗，是否为猫一系列特征，而若这一系列特征中大量样本为狗，其它动物很少，那么树在划分属性时，很容易就划分为“是否为狗”，从而产生过拟合，它不像LR等线性模型f(w,x)的正则化权重是对样本惩罚（可以实现对狗样本给与更大的惩罚项），而树的惩罚项往往是树结构相关的，因此惩罚较小，使得在高维稀疏特征时，GBDT表现不好。

#### [【务实基础】XGBoost](https://zhuanlan.zhihu.com/p/340223260)

# BatchNormalization

#### [深度学习中的Normalization模型](https://zhuanlan.zhihu.com/p/43200897)

#### [Batch Normalization导读](https://zhuanlan.zhihu.com/p/38176412)

#### [模型优化之Layer Normalization](https://zhuanlan.zhihu.com/p/54530247)

#### [NLP中 batch normalization与 layer normalization](https://zhuanlan.zhihu.com/p/74516930)

# Learning to Rank

#### [Learning to Rank简介](https://www.cnblogs.com/bentuwuying/p/6681943.html)

#### [Learning to Rank算法介绍：GBRank](https://www.cnblogs.com/bentuwuying/p/6684585.html)

#### [Learning to Rank算法介绍：RankNet，LambdaRank，LambdaMart ](https://www.cnblogs.com/bentuwuying/p/6690836.html)

#### [Learning To Rank 之 RankNet](https://blog.tsingjyujing.com/ml/recsys/ranknet)

# FM系列

#### [FM算法解析](https://zhuanlan.zhihu.com/p/37963267)

#### [推荐系统系列（一）：FM理论与实践](https://zhuanlan.zhihu.com/p/89639306)

#### [推荐系统中CTR排序模型汇总(LR,FM,FFM,MLR,DCN,deepFM,PNN,AFM等)](https://zhuanlan.zhihu.com/p/268294797)

# 深度排序

# 召回&粗排

### 框架

#### [阿里粗排技术体系与最新进展分享](https://zhuanlan.zhihu.com/p/355828527)
##### 粗排两大技术路线 
 ###### 集合选择技术
    集合选择技术是以集合为建模目标，选出满足后续链路需求的集合的方法，该技术路线在召回非常典型，这其实也非常符合粗排的定位。该方法优点是算力消耗一般比较少，缺点是依赖于对后链路的学习，可控性较弱。
  ###### 精准值预估技术
    精准值预估技术直接对最终系统目标进行精确值预估，其实也就是pointwise的方式。

#### [美团搜索粗排优化的探索与实践](https://zhuanlan.zhihu.com/p/553953132)

#### [升级换代！Facebook全新电商搜索系统Que2Search](https://zhuanlan.zhihu.com/p/428495241)

#### [KDD'21 | 揭秘Facebook升级版语义搜索技术](https://zhuanlan.zhihu.com/p/415516966)

#### [推荐系统粗排之柔：双塔 to NN，Learning to Rank](https://zhuanlan.zhihu.com/p/426679177)

#### [Google Play双塔召回算法](https://zhuanlan.zhihu.com/p/533449018)

#### [双塔召回模型的前世今生（上篇）](https://zhuanlan.zhihu.com/p/430503952)

#### [双塔召回模型的前世今生（下篇）](https://zhuanlan.zhihu.com/p/441597009)

### 样本
#### 目标: 拟合精排模型。即粗排尽可能圈出精排模型能排到前面的item。
    对于粗排模型打分比较靠前的样本（比如top5），精排模型却把这部分样本打分很低(比如top250～300)，那么，粗排打分很靠前的这几条样本是不会被曝光的，因此当我们复用精排模型的样本来训练粗排模型时，粗排模型是看不到这几条bad case的样本的，新粗排模型上线后，这几条样本很可能还被粗排模型排到靠前的位置。如果我们把这几条样本作为负样本来训练粗排模型，那么新的模型大概率就可以把这几条样本排到靠后的位置了。
####  离线训练数据的分布，应该与线上实际应用的数据，保持一致 。

排序其目标是“从用户可能喜欢的当中挑选出用户最喜欢的”，是为了优中选优。与召回相比，排序面对的数据环境，简直就是 温室里的花朵 。
召回是“是将用户可能喜欢的，和海量对用户根本不靠谱的，分隔开”，所以召回在线上所面对的数据环境，就是 鱼龙混杂、良莠不齐 。

#### [召回和粗排负样本构造问题](https://zhuanlan.zhihu.com/p/352961688)

#### [负样本为王：评Facebook的向量化召回算法](https://zhuanlan.zhihu.com/p/165064102)

#### [负样本的艺术，再读Facebook双塔向量召回算法](https://zhuanlan.zhihu.com/p/386913612)

#### [双塔模型中的负采样](https://zhuanlan.zhihu.com/p/406783325)

#### [推荐系统（四）—— 负采样](https://zhuanlan.zhihu.com/p/456088223)

### 损失函数

#### [召回粗排的损失函数loss function](https://zhuanlan.zhihu.com/p/557416100)
 为什么粗排不用精排一样的pointwise损失函数，看过网上的解释，大致概括为1、粗排不需要像精排一样预估一个有具体含义的概率值，如ctr，只需要把正负例拉开即可。那么pointwise loss也能拉开啊，还真不是：2、pointwise loss有个不足，如第500名和1000名在pointwise 设定里都是负例，label都是0，模型无法区分谁更优秀，但是pairwise的设定可以，500>1000可以作为一对有信息量的pair进行训练。有些人要说，一个有一定正例且训练充分的pointwise模型总归知道500名比1000名强吧，而且pointwise的可能训练起来更稳定好收敛易上手，不一定比pairwise出来的模型差。想知道挖掘机哪家强，还是先好好学开挖掘机

#### [随想： BPR Loss 与 Hinger Loss](https://blog.tsingjyujing.com/ml/recsys/bpr_and_hinger)

### 评价指标

#### [粗排评价指标(1)](https://zhuanlan.zhihu.com/p/340250384)

#### [看完这篇AUC文章，搞定任何有关AUC的面试不成问题~](https://zhuanlan.zhihu.com/p/360765777)

#### [机器学习的评价指标（一）：Accuracy、Precision、Recall、F1 Score](https://zhuanlan.zhihu.com/p/364253497)

#### [机器学习的评价指标（二）：ROC-AUC和PR-AUC](https://zhuanlan.zhihu.com/p/364299008)
#### [图解AUC和GAUC](https://zhuanlan.zhihu.com/p/84350940)

#### [NDCG排序评估指标](https://zhuanlan.zhihu.com/p/448686098)

### 其他

#### [双塔模型存在的问题以及解决方案【综述】](https://zhuanlan.zhihu.com/p/567981161)

# 多任务学习

#### [代码精读之MMoE](https://zhuanlan.zhihu.com/p/551786848)

#### [mmoe家族的一些summary（待续）](https://zhuanlan.zhihu.com/p/538209695)

#### [推荐系统中的多目标学习](https://zhuanlan.zhihu.com/p/183760759)

# query改写

# 对比学习

# BERT相关

#### [BERT中，multi-head 768*64*12与直接使用768*768矩阵统一计算，有什么区别？](https://www.zhihu.com/question/446385446/answer/1982483918)

```
区别在于模型容量增加，带来模型表现力的提升。
这里有一个容易引起误解的地方：“多头”不是“加头”，它实际上是“分头”
```

#### [Mask矩阵在深度学习中有哪些应用场景？](https://www.zhihu.com/question/305508138/answer/1215025036)

#### [为什么Transformer要用LayerNorm？](https://www.zhihu.com/question/487766088/answer/2129476576)

#### [transformer问题整理（参考知乎大佬内容）](https://zhuanlan.zhihu.com/p/266695736)

#### [面试准备 transformer及各种周边（待续）](https://zhuanlan.zhihu.com/p/396499248) [重点看]

#### [常用预训练语言模型（PTMs）总结](https://zhuanlan.zhihu.com/p/406512290)[重点看]

#### [史上最细节的自然语言处理NLP/Transformer/BERT/Attention面试问题与答案](https://zhuanlan.zhihu.com/p/348373259)

#### [史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！](https://zhuanlan.zhihu.com/p/148656446)

#### [超细节的BERT/Transformer知识点](https://zhuanlan.zhihu.com/p/132554155)

#### [Transformer总结(2022版)](https://zhuanlan.zhihu.com/p/489187551)

#### [BERT相关面试题（不定期更新）](https://zhuanlan.zhihu.com/p/151412524)

#### [在 BERT 已经成为 NLP 基础知识的当下，你会在面试中问关于 BERT 的哪些问题？](https://www.zhihu.com/question/424003949/answer/2626211073)

#### [原生Bert的训练和使用总结](https://zhuanlan.zhihu.com/p/163239652)

#### [Colbert论文解析](https://zhuanlan.zhihu.com/p/376475610)

#### [BERT生成式之UNILM解读](https://zhuanlan.zhihu.com/p/68327602)

#### [BERT模型蒸馏完全指南（原理/技巧/代码）](https://zhuanlan.zhihu.com/p/273378905)

#### [如何看待瘦身成功版BERT——ALBERT？](https://www.zhihu.com/question/347898375/answer/863537122)

# NLP

#### [NLP三大Subword模型详解：BPE、WordPiece、ULM](https://zhuanlan.zhihu.com/p/198964217)

# 机器学习

#### [关于损失函数和优化算法，看这一篇就够了](https://zhuanlan.zhihu.com/p/149715152)
