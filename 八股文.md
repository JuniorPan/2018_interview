# XGBoost 系列

### [机器学习-XGBoost](https://zhuanlan.zhihu.com/p/105612830)

### [深入理解XGBoost](https://zhuanlan.zhihu.com/p/83901304)

### [XGBoost 20题](https://www.cnblogs.com/cassielcode/p/12469053.html)

### [【白话机器学习】算法理论+实战之Xgboost算法](https://zhuanlan.zhihu.com/p/139635028)

[【务实基础】梯度提升树GBDT](https://zhuanlan.zhihu.com/p/338122487)

​	GBDT 缺点

- 串行方式的模型训练，难并行，造成计算开销。
- 不适合高维稀疏离散特征。这是决策树的痛点，比如动物类别采用one-hot编码后，会产生是否为狗，是否为猫一系列特征，而若这一系列特征中大量样本为狗，其它动物很少，那么树在划分属性时，很容易就划分为“是否为狗”，从而产生过拟合，它不像LR等线性模型f(w,x)的正则化权重是对样本惩罚（可以实现对狗样本给与更大的惩罚项），而树的惩罚项往往是树结构相关的，因此惩罚较小，使得在高维稀疏特征时，GBDT表现不好。

### [【务实基础】XGBoost](https://zhuanlan.zhihu.com/p/340223260)

## BatchNormalization

# Learning to Rank

### [Learning to Rank简介](https://www.cnblogs.com/bentuwuying/p/6681943.html)

### [Learning to Rank算法介绍：GBRank](https://www.cnblogs.com/bentuwuying/p/6684585.html)

### [Learning to Rank算法介绍：RankNet，LambdaRank，LambdaMart ](https://www.cnblogs.com/bentuwuying/p/6690836.html)

# FM系列

# 深度排序

### 