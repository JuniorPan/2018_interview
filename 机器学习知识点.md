

# Batch Normalization

机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的

**BatchNorm的推理过程**

BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到Mini-Batch其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。这可如何是好？这可如何是好？这可如何是好？

既然没有从Mini-Batch数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。

决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量，即：
<img src="https://pic3.zhimg.com/80/v2-63b4140ea4cb2c34e3626e963d4a527a_1440w.jpg" alt="img"  />

 **Batch Norm的四大罪状**

局限1：如果Batch Size太小，则BN效果明显下降。

局限2：对于有些像素级图片生成任务来说，BN效果不佳；

局限3：RNN等动态网络使用BN效果不佳且使用起来不方便

局限4：训练时和推理时统计量不一致





主要缺陷是依赖连续型的统计特征，对于高维度稀疏特征、时间序列特征不能很好的处理