{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch import Tensor\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, n_heads: int = 8, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model:      嵌入的维度\n",
    "            n_heads:      自注意力头的数量\n",
    "            dropout:      丢弃概率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0            # 确保头的数量可以整除嵌入的维度\n",
    "        self.d_model = d_model                   # 512 维度\n",
    "        self.n_heads = n_heads                   # 8 个头\n",
    "        self.d_key = d_model // n_heads          # 假设 d_value 等于 d_key | 512/8=64\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)    # 查询权重\n",
    "        self.Wk = nn.Linear(d_model, d_model)    # 键权重\n",
    "        self.Wv = nn.Linear(d_model, d_model)    # 值权重\n",
    "        self.Wo = nn.Linear(d_model, d_model)    # 输出权重\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)     # 初始化 dropout 层\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query:         查询向量         (batch_size, q_length, d_model)\n",
    "            key:           键向量           (batch_size, k_length, d_model)\n",
    "            value:         值向量           (batch_size, s_length, d_model)\n",
    "            mask:          解码器的掩码     \n",
    "\n",
    "        Returns:\n",
    "            output:        注意力值         (batch_size, q_length, d_model)\n",
    "            attn_probs:    softmax 分数     (batch_size, n_heads, q_length, k_length)\n",
    "        \"\"\"\n",
    "        batch_size = key.size(0)                  \n",
    "\n",
    "        # 计算查询、键和值张量\n",
    "        Q = self.Wq(query)                       # (32, 10, 512) x (512, 512) = (32, 10, 512)\n",
    "        K = self.Wk(key)                         # (32, 10, 512) x (512, 512) = (32, 10, 512)\n",
    "        V = self.Wv(value)                       # (32, 10, 512) x (512, 512) = (32, 10, 512)\n",
    "\n",
    "        # 将每个张量分割为 n 个头以计算注意力\n",
    "        # 查询张量\n",
    "        Q = Q.view(batch_size,                   \n",
    "                   -1,                           \n",
    "                   self.n_heads,              \n",
    "                   self.d_key\n",
    "                   ).permute(0, 2, 1, 3)         \n",
    "        # 键张量\n",
    "        K = K.view(batch_size,                   \n",
    "                   -1,                           \n",
    "                   self.n_heads,              \n",
    "                   self.d_key\n",
    "                   ).permute(0, 2, 1, 3)         \n",
    "        # 值张量\n",
    "        V = V.view(batch_size,                   \n",
    "                   -1,                           \n",
    "                   self.n_heads, \n",
    "                   self.d_key\n",
    "                   ).permute(0, 2, 1, 3)         \n",
    "        \n",
    "        # 计算注意力\n",
    "        # 缩放点积 -> QK^{T}\n",
    "        scaled_dot_prod = torch.matmul(Q,        \n",
    "                                       K.permute(0, 1, 3, 2)\n",
    "                                       ) / math.sqrt(self.d_key)      \n",
    "\n",
    "        # 将掩码位置为 0 的位置填充为 (-1e10)\n",
    "        if mask is not None:\n",
    "            scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # 应用 softmax \n",
    "        attn_probs = torch.softmax(scaled_dot_prod, dim=-1)\n",
    "        \n",
    "        # 乘以值以获得注意力\n",
    "        A = torch.matmul(self.dropout(attn_probs), V)       \n",
    "                                                       \n",
    "        # 将注意力重塑回 (32, 10, 512)\n",
    "        A = A.permute(0, 2, 1, 3).contiguous()              \n",
    "        A = A.view(batch_size, -1, self.n_heads*self.d_key) \n",
    "        \n",
    "        # 通过最终权重层\n",
    "        output = self.Wo(A)                                 \n",
    "\n",
    "        return output, attn_probs                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "  def __init__(self, vocab_size: int, d_model: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      vocab_size:     size of vocabulary\n",
    "      d_model:        dimension of embeddings\n",
    "    \"\"\"\n",
    "    # inherit from nn.Module\n",
    "    super().__init__()   \n",
    "     \n",
    "    # embedding look-up table (lut)                          \n",
    "    self.lut = nn.Embedding(vocab_size, d_model)   \n",
    "\n",
    "    # dimension of embeddings \n",
    "    self.d_model = d_model                          \n",
    "\n",
    "  def forward(self, x: Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x: input Tensor (batch_size, seq_length)\n",
    "      \n",
    "    Returns:\n",
    "        embedding vector\n",
    "    \"\"\"\n",
    "    # embeddings by constant sqrt(d_model)\n",
    "    # return self.lut(x) * math.sqrt(self.d_model)  \n",
    "    return self.lut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 5000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      d_model:      dimension of embeddings\n",
    "      dropout:      randomly zeroes-out some of the input\n",
    "      max_length:   max sequence length\n",
    "    \"\"\"\n",
    "    # inherit from Module\n",
    "    super().__init__()     \n",
    "\n",
    "    # initialize dropout                  \n",
    "    self.dropout = nn.Dropout(p=dropout)      \n",
    "\n",
    "    # create tensor of 0s\n",
    "    pe = torch.zeros(max_length, d_model)    \n",
    "\n",
    "    # create position column   \n",
    "    k = torch.arange(0, max_length).unsqueeze(1)  \n",
    "\n",
    "    # calc divisor for positional encoding \n",
    "    div_term = torch.exp(                                 \n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "    )\n",
    "\n",
    "    # calc sine on even indices\n",
    "    pe[:, 0::2] = torch.sin(k * div_term)    \n",
    "\n",
    "    # calc cosine on odd indices   \n",
    "    pe[:, 1::2] = torch.cos(k * div_term)  \n",
    "\n",
    "    # add dimension     \n",
    "    pe = pe.unsqueeze(0)          \n",
    "\n",
    "    # buffers are saved in state_dict but not trained by the optimizer                        \n",
    "    self.register_buffer(\"pe\", pe)                        \n",
    "\n",
    "  def forward(self, x: Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x:        embeddings (batch_size, seq_length, d_model)\n",
    "    \n",
    "    Returns:\n",
    "                embeddings + positional encodings (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    # add positional encoding to the embeddings\n",
    "    x = x + self.pe[:, : x.size(1)].requires_grad_(False) \n",
    "\n",
    "    # perform dropout\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "  def __init__(self, d_model: int, d_ffn: int, dropout: float = 0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        d_model:      dimension of embeddings\n",
    "        d_ffn:        dimension of feed-forward network\n",
    "        dropout:      probability of dropout occurring\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.w_1 = nn.Linear(d_model, d_ffn)\n",
    "    self.w_2 = nn.Linear(d_ffn, d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x:            output from attention (batch_size, seq_length, d_model)\n",
    "       \n",
    "    Returns:\n",
    "        expanded-and-contracted representation (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    # w_1(x).relu(): (batch_size, seq_length, d_model) x (d_model,d_ffn) -> (batch_size, seq_length, d_ffn)\n",
    "    # w_2(w_1(x).relu()): (batch_size, seq_length, d_ffn) x (d_ffn, d_model) -> (batch_size, seq_length, d_model) \n",
    "    return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Hello! This is an example of a paragraph that has been split into its basic components. I wonder what will come next! Any guesses?\"\n",
    "\n",
    "\n",
    "def tokenize(sequence):\n",
    "  # remove punctuation\n",
    "  for punc in [\"!\", \".\", \"?\"]:\n",
    "    sequence = sequence.replace(punc, \"\")\n",
    "  \n",
    "  # split the sequence on spaces and lowercase each token\n",
    "  return [token.lower() for token in sequence.split(\" \")]\n",
    "\n",
    "def build_vocab(data):\n",
    "  # tokenize the data and remove duplicates\n",
    "  vocab = list(set(tokenize(data)))\n",
    "\n",
    "  # sort the vocabulary\n",
    "  vocab.sort()\n",
    "\n",
    "  # assign an integer to each word\n",
    "  stoi = {word:i for i, word in enumerate(vocab)}\n",
    "\n",
    "  return stoi\n",
    "\n",
    "# build the vocab\n",
    "stoi = build_vocab(example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.22,  0.12, -0.19,  0.37, -0.54,  0.33,  0.01, -0.38],\n",
       "         [-0.36,  0.29, -0.25,  0.54, -0.41,  0.44,  0.09, -0.45],\n",
       "         [-0.27,  0.37, -0.35,  0.51, -0.57,  0.69,  0.16, -0.44],\n",
       "         [-0.19,  0.29, -0.15,  0.61, -0.41,  0.76,  0.03, -0.62],\n",
       "         [-0.31,  0.43, -0.43,  0.70, -0.86,  0.53,  0.26, -0.72],\n",
       "         [-0.29,  0.51, -0.39,  0.56, -0.43,  0.39,  0.03, -0.61]],\n",
       "\n",
       "        [[-0.44,  0.28, -0.27,  0.49, -0.53,  0.95,  0.19, -0.35],\n",
       "         [-0.60,  0.25, -0.34,  0.32, -0.43,  0.78, -0.02, -0.32],\n",
       "         [-0.42,  0.06, -0.18,  0.35, -0.61,  1.09,  0.13, -0.28],\n",
       "         [-0.55,  0.51, -0.53,  0.44, -0.63,  0.70,  0.08, -0.41],\n",
       "         [-0.51,  0.25, -0.36,  0.37, -0.47,  0.95,  0.08, -0.39],\n",
       "         [-0.53,  0.36, -0.44,  0.46, -0.55,  1.04,  0.18, -0.43]],\n",
       "\n",
       "        [[-0.54,  0.33, -0.31,  0.18, -0.33,  0.58, -0.05, -0.18],\n",
       "         [-0.65,  0.33, -0.31,  0.45, -0.54,  0.72,  0.16, -0.36],\n",
       "         [-0.79,  0.30, -0.25,  0.21, -0.34,  0.36, -0.12, -0.25],\n",
       "         [-0.58,  0.06, -0.12,  0.38, -0.75,  0.73,  0.10, -0.31],\n",
       "         [-0.54,  0.18, -0.19,  0.47, -0.75,  0.66,  0.13, -0.42],\n",
       "         [-0.69,  0.38, -0.24,  0.63, -0.37,  0.67,  0.10, -0.51]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "\n",
    "# convert the sequences to integers\n",
    "sequences = [\"I wonder what will come next!\",\n",
    "             \"This is a basic example paragraph.\",\n",
    "             \"Hello what is a basic split?\"]\n",
    "\n",
    "# tokenize the sequences\n",
    "tokenized_sequences = [tokenize(seq) for seq in sequences]\n",
    "\n",
    "# index the sequences \n",
    "indexed_sequences = [[stoi[word] for word in seq] for seq in tokenized_sequences]\n",
    "\n",
    "# convert the sequences to a tensor\n",
    "tensor_sequences = torch.tensor(indexed_sequences).long()\n",
    "\n",
    "# vocab size\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "# embedding dimensions\n",
    "d_model = 8\n",
    "\n",
    "# create the embeddings\n",
    "lut = Embeddings(vocab_size, d_model) # look-up table (lut)\n",
    "\n",
    "# create the positional encodings\n",
    "pe = PositionalEncoding(d_model=d_model, dropout=0.1, max_length=10)\n",
    "\n",
    "# embed the sequence\n",
    "embeddings = lut(tensor_sequences)\n",
    "\n",
    "# positionally encode the sequences\n",
    "X = pe(embeddings)\n",
    "\n",
    "# set the n_heads\n",
    "n_heads = 4\n",
    "\n",
    "# create the attention layer\n",
    "attention = MultiHeadAttention(d_model, n_heads, dropout=0.1)\n",
    "\n",
    "# pass X through the attention layer three times to create Q, K, and V\n",
    "output, attn_probs = attention(X, X, X, mask=None)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):  \n",
    "  def __init__(self, d_model: int, n_heads: int, d_ffn: int, dropout: float):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        d_model:      dimension of embeddings\n",
    "        n_heads:      number of heads\n",
    "        d_ffn:        dimension of feed-forward network\n",
    "        dropout:      probability of dropout occurring\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    # multi-head attention sublayer\n",
    "    self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "    # layer norm for multi-head attention\n",
    "    self.attn_layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    # position-wise feed-forward network\n",
    "    self.positionwise_ffn = PositionwiseFeedForward(d_model, d_ffn, dropout)\n",
    "    # layer norm for position-wise ffn\n",
    "    self.ffn_layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src: Tensor, src_mask: Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        src:          positionally embedded sequences   (batch_size, seq_length, d_model)\n",
    "        src_mask:     mask for the sequences            (batch_size, 1, 1, seq_length)\n",
    "    Returns:\n",
    "        src:          sequences after self-attention    (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    # pass embeddings through multi-head attention\n",
    "    _src, attn_probs = self.attention(src, src, src, src_mask)\n",
    "\n",
    "    # residual add and norm\n",
    "    src = self.attn_layer_norm(src + self.dropout(_src))\n",
    "    \n",
    "    # position-wise feed-forward network\n",
    "    _src = self.positionwise_ffn(src)\n",
    "\n",
    "    # residual add and norm\n",
    "    src = self.ffn_layer_norm(src + self.dropout(_src)) \n",
    "\n",
    "    return src, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, d_model: int, n_layers: int, \n",
    "               n_heads: int, d_ffn: int, dropout: float = 0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        d_model:      dimension of embeddings\n",
    "        n_layers:     number of encoder layers\n",
    "        n_heads:      number of heads\n",
    "        d_ffn:        dimension of feed-forward network\n",
    "        dropout:      probability of dropout occurring\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    \n",
    "    # create n_layers encoders \n",
    "    self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ffn, dropout)\n",
    "                                 for layer in range(n_layers)])\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "  def forward(self, src: Tensor, src_mask: Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        src:          embedded sequences                (batch_size, seq_length, d_model)\n",
    "        src_mask:     mask for the sequences            (batch_size, 1, 1, seq_length)\n",
    "\n",
    "    Returns:\n",
    "        src:          sequences after self-attention    (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    # pass the sequences through each encoder\n",
    "    for layer in self.layers:\n",
    "      src, attn_probs = layer(src, src_mask)\n",
    "\n",
    "    self.attn_probs = attn_probs\n",
    "\n",
    "    return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.42,  0.82, -0.81,  1.10, -0.97,  1.50, -0.45,  0.23],\n",
       "         [ 0.03,  1.52, -0.30,  1.75, -1.09, -0.39, -0.68, -0.84],\n",
       "         [ 0.28,  0.09,  1.82,  0.64, -1.12,  0.43, -0.57, -1.56],\n",
       "         [-0.15, -1.15,  1.07,  1.81, -1.20,  0.52, -0.72, -0.18],\n",
       "         [-0.36, -0.07,  1.23,  1.02, -1.28,  0.48,  0.69, -1.72],\n",
       "         [-1.16,  0.01,  1.55,  1.55, -1.22, -0.13, -0.02, -0.59]],\n",
       "\n",
       "        [[ 1.54,  0.73, -0.90,  0.16, -0.18, -0.03, -1.96,  0.62],\n",
       "         [ 1.14,  0.61,  0.81,  0.02, -0.26, -0.29,  0.30, -2.33],\n",
       "         [ 1.54,  1.22, -0.83, -0.30,  0.51,  0.12, -1.64, -0.63],\n",
       "         [ 2.17, -0.58,  0.44, -0.22,  0.72, -1.02, -0.70, -0.81],\n",
       "         [ 0.62, -1.29,  0.30,  0.15,  0.93,  0.13, -1.94,  1.10],\n",
       "         [-0.17,  1.08, -0.84,  1.64,  0.38,  0.26, -0.72, -1.63]],\n",
       "\n",
       "        [[-1.18,  0.43,  0.39,  1.32, -1.75,  1.08,  0.20, -0.48],\n",
       "         [ 0.44,  0.58,  1.83, -0.20, -0.47,  0.50, -1.46, -1.22],\n",
       "         [ 0.94, -0.33,  1.51,  0.44, -1.04,  0.12,  0.22, -1.85],\n",
       "         [ 0.28,  0.04, -1.47, -1.40,  1.51,  1.22, -0.17, -0.01],\n",
       "         [-0.02, -0.19,  1.06,  1.59, -1.72, -0.01,  0.39, -1.09],\n",
       "         [-0.60,  1.86, -0.19,  1.32, -1.24,  0.11, -0.80, -0.45]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "\n",
    "# convert the sequences to integers\n",
    "sequences = [\"I wonder what will come next!\",\n",
    "             \"This is a basic example paragraph.\",\n",
    "             \"Hello what is a basic split?\"]\n",
    "\n",
    "# tokenize the sequences\n",
    "tokenized_sequences = [tokenize(seq) for seq in sequences]\n",
    "\n",
    "# index the sequences \n",
    "indexed_sequences = [[stoi[word] for word in seq] for seq in tokenized_sequences]\n",
    "\n",
    "# convert the sequences to a tensor\n",
    "tensor_sequences = torch.tensor(indexed_sequences).long()\n",
    "\n",
    "# parameters\n",
    "vocab_size = len(stoi)\n",
    "d_model = 8\n",
    "d_ffn = d_model*4 # 32\n",
    "n_heads = 4\n",
    "n_layers = 4\n",
    "dropout = 0.1\n",
    "\n",
    "# create the embeddings\n",
    "lut = Embeddings(vocab_size, d_model) # look-up table (lut)\n",
    "\n",
    "# create the positional encodings\n",
    "pe = PositionalEncoding(d_model=d_model, dropout=0.1, max_length=10)\n",
    "\n",
    "# embed the sequence\n",
    "embeddings = lut(tensor_sequences)\n",
    "\n",
    "# positionally encode the sequences\n",
    "X = pe(embeddings)\n",
    "\n",
    "# initialize encoder\n",
    "encoder = Encoder(d_model, n_layers, n_heads,\n",
    "                  d_ffn, dropout)\n",
    "\n",
    "# pass through encoder\n",
    "encoder(src=X, src_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
